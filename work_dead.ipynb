{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "monet_dir = Path(\"dataset_monet\")\n",
    "photo_dir = Path(\"dataset_photo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_dataset = Dataset(monet_dir)\n",
    "data_loader = DataLoader(monet_dataset, BATCH_SIZE, shuffle=True, num_workers=N_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 files belonging to 1 classes.\n",
      "Using 240 files for training.\n",
      "Found 300 files belonging to 1 classes.\n",
      "Using 60 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_monet = tf.keras.utils.image_dataset_from_directory(\n",
    "  monet_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "  \n",
    "test_monet = tf.keras.utils.image_dataset_from_directory(\n",
    "  monet_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monet_dir = \"dataset_monet\\monet_jpg\"\n",
    "# photo_dir = \"dataset_photo\\photo_jpg\"\n",
    "\n",
    "# all_photo = tf.data.Dataset.list_files(photo_dir + '\\\\*jpg')\n",
    "# all_monet = tf.data.Dataset.list_files(monet_dir + '\\\\*jpg')\n",
    "\n",
    "# # def load(filepath):\n",
    "# #     image = tf.io.read_file(filepath)\n",
    "# #     image = tf.image.decode_image(image)\n",
    "# #     return image\n",
    "\n",
    "# # ds = files.map(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_photo = all_photo.take(5631) \n",
    "# test_photo = all_photo.skip(1407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_monet = all_monet.take(240) \n",
    "# test_monet = all_monet.skip(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7038 files belonging to 1 classes.\n",
      "Using 5631 files for training.\n",
      "Found 7038 files belonging to 1 classes.\n",
      "Using 1407 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_photo = tf.keras.utils.image_dataset_from_directory(\n",
    "  photo_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "  \n",
    "test_photo = tf.keras.utils.image_dataset_from_directory(\n",
    "  photo_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "<BatchDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "<BatchDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "<BatchDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(train_photo)\n",
    "print(train_monet)\n",
    "print(test_photo)\n",
    "print(test_monet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 1000\n",
    "# def random_crop(image):\n",
    "#   cropped_image = tf.image.random_crop(\n",
    "#       image, size=[img_height, img_width, 3])\n",
    "\n",
    "#   return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalizing the images to [-1, 1]\n",
    "# def normalize(image):\n",
    "#   image = tf.cast(image, tf.float32)\n",
    "#   image = (image / 127.5) - 1\n",
    "#   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_jitter(image):\n",
    "#   # resizing to 286 x 286 x 3\n",
    "#   image = tf.image.resize(image, [286, 286],\n",
    "#                           method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "#   # randomly cropping to 256 x 256 x 3\n",
    "#   image = random_crop(image)\n",
    "\n",
    "#   # random mirroring\n",
    "#   image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "#   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_image_train(image):\n",
    "#   image = random_jitter(image)\n",
    "#   image = normalize(image)\n",
    "#   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_image_test(image):\n",
    "#   image = normalize(image)\n",
    "#   return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\CEPC~1\\AppData\\Local\\Temp/ipykernel_10384/2038447156.py\", line 2, in preprocess_image_train  *\n        image = random_jitter(image)\n    File \"C:\\Users\\CEPC~1\\AppData\\Local\\Temp/ipykernel_10384/2097116048.py\", line 3, in random_jitter  *\n        image = tf.image.resize(image, [286, 286],\n\n    ValueError: 'images' must have either 3 or 4 dimensions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CEPC~1\\AppData\\Local\\Temp/ipykernel_10384/577741202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_monet = train_monet.cache().map(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpreprocess_image_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     BUFFER_SIZE).batch(batch_size)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m train_photo = train_photo.cache().map(\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2004\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2005\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2006\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   2007\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5499\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5500\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5501\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   5502\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4533\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4534\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3242\u001b[0m          \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3243\u001b[0m     \"\"\"\n\u001b[1;32m-> 3244\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   3245\u001b[0m         *args, **kwargs)\n\u001b[0;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3210\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3390\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3391\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3392\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3393\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3394\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4508\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   4509\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4510\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4511\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4512\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4438\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4439\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4440\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4441\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4442\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    697\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\CEPC~1\\AppData\\Local\\Temp/ipykernel_10384/2038447156.py\", line 2, in preprocess_image_train  *\n        image = random_jitter(image)\n    File \"C:\\Users\\CEPC~1\\AppData\\Local\\Temp/ipykernel_10384/2097116048.py\", line 3, in random_jitter  *\n        image = tf.image.resize(image, [286, 286],\n\n    ValueError: 'images' must have either 3 or 4 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# train_monet = train_monet.cache().map(\n",
    "#     preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "#     BUFFER_SIZE).batch(batch_size)\n",
    "\n",
    "# train_photo = train_photo.cache().map(\n",
    "#     preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "#     BUFFER_SIZE).batch(batch_size)\n",
    "\n",
    "# test_monet = test_monet.map(\n",
    "#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "#     BUFFER_SIZE).batch(batch_size)\n",
    "\n",
    "# test_photo = test_photo.map(\n",
    "#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "#     BUFFER_SIZE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_monet = next(iter(train_monet))\n",
    "sample_photo = next(iter(train_photo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"model_4\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(1, 256, 256, 3), dtype=float32, numpy=\narray([[[[ 87., 142., 199.],\n         [ 87., 142., 199.],\n         [ 87., 142., 199.],\n         ...,\n         [ 88., 144., 201.],\n         [ 89., 145., 202.],\n         [ 90., 146., 203.]],\n\n        [[ 89., 144., 201.],\n         [ 89., 144., 201.],\n         [ 89., 144., 201.],\n         ...,\n         [ 89., 145., 202.],\n         [ 89., 145., 202.],\n         [ 90., 146., 203.]],\n\n        [[ 91., 146., 202.],\n         [ 91., 146., 202.],\n         [ 91., 146., 202.],\n         ...,\n         [ 89., 145., 202.],\n         [ 89., 145., 202.],\n         [ 90., 146., 203.]],\n\n        ...,\n\n        [[ 50.,  54.,  57.],\n         [ 62.,  66.,  69.],\n         [ 34.,  38.,  39.],\n         ...,\n         [172., 174., 169.],\n         [150., 156., 152.],\n         [198., 207., 202.]],\n\n        [[ 46.,  49.,  54.],\n         [ 66.,  70.,  73.],\n         [ 74.,  78.,  79.],\n         ...,\n         [152., 154., 143.],\n         [152., 162., 154.],\n         [145., 160., 155.]],\n\n        [[ 29.,  32.,  37.],\n         [ 33.,  37.,  40.],\n         [ 54.,  58.,  61.],\n         ...,\n         [161., 163., 150.],\n         [154., 165., 157.],\n         [151., 168., 162.]]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CEPC~1\\AppData\\Local\\Temp/ipykernel_10384/3617849715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mto_monet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_g\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_photo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mto_photo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_monet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcontrast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ce pc\\Documents\\EMSE\\DSC\\DL\\Projet\\S9DeepLearning\\.venv\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\u001b[0m\u001b[0;32m    200\u001b[0m                      \u001b[1;34mf' but it received {len(inputs)} input tensors. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                      f'Inputs received: {inputs}')\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"model_4\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(1, 256, 256, 3), dtype=float32, numpy=\narray([[[[ 87., 142., 199.],\n         [ 87., 142., 199.],\n         [ 87., 142., 199.],\n         ...,\n         [ 88., 144., 201.],\n         [ 89., 145., 202.],\n         [ 90., 146., 203.]],\n\n        [[ 89., 144., 201.],\n         [ 89., 144., 201.],\n         [ 89., 144., 201.],\n         ...,\n         [ 89., 145., 202.],\n         [ 89., 145., 202.],\n         [ 90., 146., 203.]],\n\n        [[ 91., 146., 202.],\n         [ 91., 146., 202.],\n         [ 91., 146., 202.],\n         ...,\n         [ 89., 145., 202.],\n         [ 89., 145., 202.],\n         [ 90., 146., 203.]],\n\n        ...,\n\n        [[ 50.,  54.,  57.],\n         [ 62.,  66.,  69.],\n         [ 34.,  38.,  39.],\n         ...,\n         [172., 174., 169.],\n         [150., 156., 152.],\n         [198., 207., 202.]],\n\n        [[ 46.,  49.,  54.],\n         [ 66.,  70.,  73.],\n         [ 74.,  78.,  79.],\n         ...,\n         [152., 154., 143.],\n         [152., 162., 154.],\n         [145., 160., 155.]],\n\n        [[ 29.,  32.,  37.],\n         [ 33.,  37.,  40.],\n         [ 54.,  58.,  61.],\n         ...,\n         [161., 163., 150.],\n         [154., 165., 157.],\n         [151., 168., 162.]]]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>]"
     ]
    }
   ],
   "source": [
    "to_monet = generator_g(sample_photo)\n",
    "to_photo = generator_f(sample_monet)\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [sample_photo, to_monet, sample_monet, to_photo]\n",
    "title = ['photo', 'To monet', 'monet', 'To photo']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.title(title[i])\n",
    "  if i % 2 == 0:\n",
    "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
    "  else:\n",
    "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bd15faaca0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00185516]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c42e67fac1dbbb905bab158e8d1c73c2bb2f0556c742ed3c1778ccdf61602368"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
